{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U4wK_1Sp5oZz"
   },
   "outputs": [],
   "source": [
    "#importing required libraries and packages\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "ff7CdOKkPise",
    "outputId": "31da2687-553d-4f2d-e280-8892902d7212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is: 1.1.5\n",
      "numpy version is: 1.19.5\n",
      "matplotlib version is: 3.2.2\n",
      "sklearn version is: 0.22.2.post1\n",
      "xgboost version is: 0.90\n"
     ]
    }
   ],
   "source": [
    "#printing out versions of all packages and libraries and used\n",
    "\n",
    "print(f'pandas version is: {pd.__version__}')\n",
    "print(f'numpy version is: {np.__version__}')\n",
    "print(f'matplotlib version is: {matplotlib.__version__}')\n",
    "print(f'sklearn version is: {sklearn.__version__}')\n",
    "print(f'xgboost version is: {xgb.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z1HHu00J6gBZ"
   },
   "outputs": [],
   "source": [
    "#all helper functions used\n",
    "\n",
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    data = data.drop(columns, axis=1)\n",
    "        \n",
    "    return data\n",
    " \n",
    "def process(data):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "    \n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "    \n",
    "    cols = list(data.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        data[_] = np.where(data[_] == np.inf, -999, data[_])\n",
    "        data[_] = np.where(data[_] == np.nan, -999, data[_])\n",
    "        data[_] = np.where(data[_] == -np.inf, -999, data[_])\n",
    "        \n",
    "    return data\n",
    " \n",
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
    "\n",
    "\n",
    "#Paulo Bestagini's feature augmentation technique from SEG 2016 ML competition\n",
    "#Link : https://github.com/seg/2016-ml-contest/tree/master/ispl\n",
    "\n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    " \n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    " \n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    " \n",
    "    return X_aug\n",
    " \n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    " \n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SMZi5ywWUCQV",
    "outputId": "1718faa6-eea7-4e6a-bda8-6174cd9039b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/utpdsh/FORCE-2020-Lithology/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kSQfmlIWxBM8"
   },
   "outputs": [],
   "source": [
    "#should be edited to the present working directory of the user\n",
    "PWD = '/content/drive/MyDrive/utpdsh/FORCE-2020-Lithology/notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1cLS1yO56iT_"
   },
   "outputs": [],
   "source": [
    "#importing penaltry matrix used for evaluation and train and test files\n",
    "A = np.load(PWD + 'penalty_matrix.npy')\n",
    "\n",
    "train = pd.read_csv(PWD + 'train.csv')\n",
    "\n",
    "test = pd.read_csv(PWD + 'test.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL</th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>SGR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>PEF</th>\n",
       "      <th>DTC</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DTS</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>RXO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/9-14</td>\n",
       "      <td>480.628001</td>\n",
       "      <td>423244.50000</td>\n",
       "      <td>6461862.5</td>\n",
       "      <td>-455.624420</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.613886</td>\n",
       "      <td>1.513937</td>\n",
       "      <td>2.171173</td>\n",
       "      <td>72.078377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551575</td>\n",
       "      <td>55.815659</td>\n",
       "      <td>174.347092</td>\n",
       "      <td>35.525719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.461990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.538873</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15/9-14</td>\n",
       "      <td>480.780001</td>\n",
       "      <td>423244.50000</td>\n",
       "      <td>6461862.5</td>\n",
       "      <td>-455.776428</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.574376</td>\n",
       "      <td>1.480188</td>\n",
       "      <td>2.171840</td>\n",
       "      <td>72.147697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.547566</td>\n",
       "      <td>41.862537</td>\n",
       "      <td>176.112915</td>\n",
       "      <td>36.158520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.454399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.539232</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15/9-14</td>\n",
       "      <td>480.932001</td>\n",
       "      <td>423244.50000</td>\n",
       "      <td>6461862.5</td>\n",
       "      <td>-455.928436</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.436627</td>\n",
       "      <td>1.469700</td>\n",
       "      <td>2.161303</td>\n",
       "      <td>75.074219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551840</td>\n",
       "      <td>38.587872</td>\n",
       "      <td>177.662476</td>\n",
       "      <td>36.873703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.446686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.540830</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15/9-14</td>\n",
       "      <td>481.084001</td>\n",
       "      <td>423244.50000</td>\n",
       "      <td>6461862.5</td>\n",
       "      <td>-456.080444</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.276094</td>\n",
       "      <td>1.469341</td>\n",
       "      <td>2.151947</td>\n",
       "      <td>80.735229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.552511</td>\n",
       "      <td>41.417999</td>\n",
       "      <td>177.437714</td>\n",
       "      <td>37.304054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.170166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.543943</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/9-14</td>\n",
       "      <td>481.236001</td>\n",
       "      <td>423244.53125</td>\n",
       "      <td>6461862.5</td>\n",
       "      <td>-456.232422</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.204704</td>\n",
       "      <td>1.466462</td>\n",
       "      <td>2.143010</td>\n",
       "      <td>81.665024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566789</td>\n",
       "      <td>49.471237</td>\n",
       "      <td>176.797653</td>\n",
       "      <td>37.864922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>172.489120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.542104</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136781</th>\n",
       "      <td>35/9-8</td>\n",
       "      <td>3224.389600</td>\n",
       "      <td>536225.93750</td>\n",
       "      <td>6794880.5</td>\n",
       "      <td>-3199.876465</td>\n",
       "      <td>BAAT GP.</td>\n",
       "      <td>Rannoch Fm.</td>\n",
       "      <td>8.4978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.231942</td>\n",
       "      <td>7.027874</td>\n",
       "      <td>2.582710</td>\n",
       "      <td>60.644516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.178304</td>\n",
       "      <td>5.580282</td>\n",
       "      <td>67.790977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>26.615782</td>\n",
       "      <td>118.669212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.618309</td>\n",
       "      <td>33.523922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136782</th>\n",
       "      <td>35/9-8</td>\n",
       "      <td>3224.541600</td>\n",
       "      <td>536225.93750</td>\n",
       "      <td>6794880.5</td>\n",
       "      <td>-3200.028320</td>\n",
       "      <td>BAAT GP.</td>\n",
       "      <td>Rannoch Fm.</td>\n",
       "      <td>8.4978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.038777</td>\n",
       "      <td>6.875511</td>\n",
       "      <td>2.578110</td>\n",
       "      <td>60.317150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179475</td>\n",
       "      <td>5.532178</td>\n",
       "      <td>67.895126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>25.647141</td>\n",
       "      <td>118.468925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.620221</td>\n",
       "      <td>32.643795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136783</th>\n",
       "      <td>35/9-8</td>\n",
       "      <td>3224.693600</td>\n",
       "      <td>536225.93750</td>\n",
       "      <td>6794880.5</td>\n",
       "      <td>-3200.180176</td>\n",
       "      <td>BAAT GP.</td>\n",
       "      <td>Rannoch Fm.</td>\n",
       "      <td>8.4978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.503983</td>\n",
       "      <td>6.338658</td>\n",
       "      <td>2.578331</td>\n",
       "      <td>63.907867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181352</td>\n",
       "      <td>4.766746</td>\n",
       "      <td>68.217995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>23.929407</td>\n",
       "      <td>118.163177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.629171</td>\n",
       "      <td>31.763380</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136784</th>\n",
       "      <td>35/9-8</td>\n",
       "      <td>3224.845600</td>\n",
       "      <td>536225.93750</td>\n",
       "      <td>6794880.5</td>\n",
       "      <td>-3200.332031</td>\n",
       "      <td>BAAT GP.</td>\n",
       "      <td>Rannoch Fm.</td>\n",
       "      <td>8.4978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.895551</td>\n",
       "      <td>5.666211</td>\n",
       "      <td>2.568203</td>\n",
       "      <td>64.572060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182943</td>\n",
       "      <td>4.723001</td>\n",
       "      <td>68.701576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>22.737293</td>\n",
       "      <td>117.655937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.521121</td>\n",
       "      <td>30.884350</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136785</th>\n",
       "      <td>35/9-8</td>\n",
       "      <td>3224.997600</td>\n",
       "      <td>536225.93750</td>\n",
       "      <td>6794880.5</td>\n",
       "      <td>-3200.483887</td>\n",
       "      <td>BAAT GP.</td>\n",
       "      <td>Rannoch Fm.</td>\n",
       "      <td>8.4978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.564547</td>\n",
       "      <td>5.261881</td>\n",
       "      <td>2.553497</td>\n",
       "      <td>69.898911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.183442</td>\n",
       "      <td>4.687353</td>\n",
       "      <td>69.349098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>21.116669</td>\n",
       "      <td>117.002289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.356035</td>\n",
       "      <td>29.962900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136786 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           WELL     DEPTH_MD         X_LOC  ...      RMIC       ROPA RXO\n",
       "0       15/9-14   480.628001  423244.50000  ...       NaN        NaN NaN\n",
       "1       15/9-14   480.780001  423244.50000  ...       NaN        NaN NaN\n",
       "2       15/9-14   480.932001  423244.50000  ...       NaN        NaN NaN\n",
       "3       15/9-14   481.084001  423244.50000  ...       NaN        NaN NaN\n",
       "4       15/9-14   481.236001  423244.53125  ...       NaN        NaN NaN\n",
       "...         ...          ...           ...  ...       ...        ...  ..\n",
       "136781   35/9-8  3224.389600  536225.93750  ...  2.618309  33.523922 NaN\n",
       "136782   35/9-8  3224.541600  536225.93750  ...  2.620221  32.643795 NaN\n",
       "136783   35/9-8  3224.693600  536225.93750  ...  2.629171  31.763380 NaN\n",
       "136784   35/9-8  3224.845600  536225.93750  ...  2.521121  30.884350 NaN\n",
       "136785   35/9-8  3224.997600  536225.93750  ...  2.356035  29.962900 NaN\n",
       "\n",
       "[136786 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xQSZM0glXhPD"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    '''\n",
    "    class to lithology prediction\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "\n",
    "        '''\n",
    "        takes in the train and test dataframes\n",
    "        '''\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "        \n",
    "    def __call__(self, plot = True):\n",
    "\n",
    "      return self.fit(plot)\n",
    "\n",
    "    def preprocess(self, train, test):\n",
    "\n",
    "        '''\n",
    "        method to prepare datasets for training and predictions\n",
    "        accepts both the train and test dataframes as arguments\n",
    "\n",
    "        returns the prepared train, test datasets along with the\n",
    "        lithology labels and numbers which is needed for preparing\n",
    "        the submission file\n",
    "\n",
    "        '''\n",
    "\n",
    "        #concatenating both train and test datasets for easier and uniform processing\n",
    "\n",
    "        ntrain = train.shape[0]\n",
    "        ntest = test.shape[0]\n",
    "        target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
    "        df = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "        #mapping the lithology labels to ordinal values for better modelling\n",
    "\n",
    "        lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    " \n",
    "        lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "        \n",
    "        lithology1 = lithology.map(lithology_numbers)\n",
    "\n",
    "        #implementing Bestagini's augmentation procedure\n",
    "\n",
    "        train_well = train.WELL.values\n",
    "        train_depth = train.DEPTH_MD.values\n",
    "        \n",
    "        test_well = test.WELL.values\n",
    "        test_depth = test.DEPTH_MD.values  \n",
    "        '''to be continued...\n",
    "        #this was done here for ease as the datasets would undergo some transformations\n",
    "        #that would make it uneasy to perform the augmentation technique'''\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'shape of concatenated dataframe before dropping columns {df.shape}')\n",
    "\n",
    "        cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped\n",
    "        df = drop_columns(df, *cols)\n",
    "        print(f'shape of dataframe after dropping columns {df.shape}')\n",
    "        print(f'{cols} were dropped')\n",
    "\n",
    "        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations\n",
    "\n",
    "        df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
    "        df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "        df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "        df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "        print(f'shape of dataframe after label encoding columns {df.shape}')\n",
    "\n",
    "\n",
    "        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION\n",
    "        print(f'Splitting concatenated dataframe into training and test datasets...')\n",
    "        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
    "        print(df.shape)\n",
    "        \n",
    "        df = df.fillna(-999)\n",
    "        df = process(df)\n",
    "        data = df.copy()\n",
    "        \n",
    "        train2 = data[:ntrain].copy()\n",
    "        train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        \n",
    "        test2 = data[ntrain:(ntest+ntrain)].copy()\n",
    "        test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        test2 = test2.reset_index(drop=True)\n",
    "\n",
    "        traindata = train2\n",
    "        testdata = test2\n",
    "\n",
    "        print(f'Shape of train and test datasets before augmentation {traindata.shape, testdata.shape}')\n",
    " \n",
    "        traindata1, padded_rows = augment_features(pd.DataFrame(traindata).values, train_well, train_depth)\n",
    "        testdata1, padded_rows = augment_features(pd.DataFrame(testdata).values, test_well, test_depth)\n",
    "        \n",
    "        print(f'Shape of train and test datasets after augmentation {traindata1.shape, testdata1.shape}')\n",
    "    \n",
    "        return traindata1, testdata1, lithology1, lithology_numbers\n",
    "\n",
    "    \n",
    "    def fit(self, plot):\n",
    "\n",
    "      '''\n",
    "      method to train model and make predictions\n",
    "\n",
    "      returns the test predictions, trained model, and lithology numbers used for making the submission file\n",
    "      '''\n",
    "\n",
    "      traindata1, testdata1, lithology1, lithology_numbers = self.preprocess(self.train, self.test)\n",
    "\n",
    "      #using a 10-fold stratified cross-validation technique and seting the shuffle parameter to true\n",
    "      #as this improved the validation performance better\n",
    "\n",
    "      split = 10\n",
    "      kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "  \n",
    "      open_test = np.zeros((len(testdata1), 12))\n",
    "      \n",
    "      #100 n-estimators and 10 max-depth\n",
    "      model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                            objective='multi:softprob', learning_rate=0.1, random_state=0,\n",
    "                            subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                            eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n",
    "      \n",
    " \n",
    "      i = 1\n",
    "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata1), pd.DataFrame(lithology1)):\n",
    "        X_train, X_test = pd.DataFrame(traindata1).iloc[train_index], pd.DataFrame(traindata1).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(lithology1).iloc[train_index],pd.DataFrame(lithology1).iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
    "        prediction = model.predict(X_test)\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    " \n",
    "        print(f'-----------------------FOLD {i}---------------------')\n",
    "        i+=1\n",
    " \n",
    "        open_test += model.predict_proba(pd.DataFrame(testdata1))\n",
    "      \n",
    "      open_test= pd.DataFrame(open_test/split)\n",
    "    \n",
    "      open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
    " \n",
    "      print('---------------CROSS VALIDATION COMPLETE')\n",
    "      print('----------------TEST EVALUATION------------------')\n",
    "\n",
    "                  \n",
    "      if plot: self.plot_feat_imp(model)\n",
    "      return open_test, model, lithology_numbers\n",
    "              \n",
    "              \n",
    "    def plot_feat_imp(self, model):\n",
    "        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "\n",
    "    def make_submission_file(self, filename):\n",
    "\n",
    "      '''\n",
    "      method to train model, make prediction and create submission file\n",
    "      args::\n",
    "        filename: name to save submission file as (string)\n",
    "      '''\n",
    "\n",
    "      self.filename = filename\n",
    "\n",
    "      prediction, model, lithology_numbers = self.fit(plot=False)\n",
    " \n",
    "      path = PWD\n",
    "    \n",
    "      test = pd.read_csv(PWD + 'test.csv', sep=';')\n",
    "      \n",
    "      category_to_lithology = {y:x for x,y in lithology_numbers.items()}\n",
    "      test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)\n",
    "      np.savetxt(path+filename+'.csv', test_prediction_for_submission, header='lithology', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t3dOrluJWtPM",
    "outputId": "57b1421a-85db-41e2-85d8-4cfcbeec915c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of concatenated dataframe before dropping columns (1307297, 29)\n",
      "shape of dataframe after dropping columns (1307297, 24)\n",
      "['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "shape of dataframe after label encoding columns (1307297, 27)\n",
      "Splitting concatenated dataframe into training and test datasets...\n",
      "(1307297, 24)\n",
      "Shape of train and test datasets before augmentation ((1170511, 23), (136786, 23))\n",
      "Shape of train and test datasets after augmentation ((1170511, 92), (136786, 92))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.16355\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.318913\n",
      "Default score: [-0.27682248]\n",
      "Accuracy is: 0.8949868434541913\n",
      "F1 is: 0.8989404049897284\n",
      "None\n",
      "-----------------------FOLD 1---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16484\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.3181\n",
      "Default score: [-0.27791839]\n",
      "Accuracy is: 0.894302483532819\n",
      "F1 is: 0.898170670837942\n",
      "None\n",
      "-----------------------FOLD 2---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16457\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.320091\n",
      "Default score: [-0.28001576]\n",
      "Accuracy is: 0.8933883520858429\n",
      "F1 is: 0.8975193439855438\n",
      "None\n",
      "-----------------------FOLD 3---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16415\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.318961\n",
      "Default score: [-0.27797071]\n",
      "Accuracy is: 0.8944648059392915\n",
      "F1 is: 0.8983362346019375\n",
      "None\n",
      "-----------------------FOLD 4---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.1642\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319949\n",
      "Default score: [-0.27932376]\n",
      "Accuracy is: 0.8937984297442995\n",
      "F1 is: 0.8978087743267712\n",
      "None\n",
      "-----------------------FOLD 5---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16431\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319644\n",
      "Default score: [-0.27732249]\n",
      "Accuracy is: 0.8943622865246773\n",
      "F1 is: 0.8984362538507427\n",
      "None\n",
      "-----------------------FOLD 6---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16494\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.321131\n",
      "Default score: [-0.28100678]\n",
      "Accuracy is: 0.89329437595578\n",
      "F1 is: 0.897325696468155\n",
      "None\n",
      "-----------------------FOLD 7---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16419\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.321098\n",
      "Default score: [-0.27910163]\n",
      "Accuracy is: 0.8941914208336537\n",
      "F1 is: 0.8981240171484763\n",
      "None\n",
      "-----------------------FOLD 8---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16411\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.317646\n",
      "Default score: [-0.27334025]\n",
      "Accuracy is: 0.8961905494186295\n",
      "F1 is: 0.900029410617408\n",
      "None\n",
      "-----------------------FOLD 9---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16445\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.318521\n",
      "Default score: [-0.27869369]\n",
      "Accuracy is: 0.8937300834678901\n",
      "F1 is: 0.89752519615268\n",
      "None\n",
      "-----------------------FOLD 10---------------------\n",
      "---------------CROSS VALIDATION COMPLETE\n",
      "----------------TEST EVALUATION------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To train model and make prediction\n",
    "\n",
    "func_= Model(train, test)\n",
    "prediction, model, redundant = func_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sueoRdaykxzp",
    "outputId": "c62c299c-d239-4ea5-cf41-0dc53ea27b11"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3d8c93f720f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Fd45KJLVk3Ka"
   },
   "outputs": [],
   "source": [
    "#initializing the model class\n",
    "\n",
    "func_= Model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zRywwBE3mCb7",
    "outputId": "b03891b3-6800-465d-e0c0-8e75a2774382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of concatenated dataframe before dropping columns (1307297, 29)\n",
      "shape of dataframe after dropping columns (1307297, 24)\n",
      "['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "shape of dataframe after label encoding columns (1307297, 27)\n",
      "Splitting concatenated dataframe into training and test datasets...\n",
      "(1307297, 24)\n",
      "Shape of train and test datasets before augmentation ((1170511, 23), (136786, 23))\n",
      "Shape of train and test datasets after augmentation ((1170511, 92), (136786, 92))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.16424\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.315742\n",
      "Default score: [-0.27532208]\n",
      "Accuracy is: 0.895328571916755\n",
      "F1 is: 0.8991049294557178\n",
      "None\n",
      "-----------------------FOLD 1---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16408\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.316871\n",
      "Default score: [-0.27342996]\n",
      "Accuracy is: 0.8960538568658106\n",
      "F1 is: 0.8997973787999167\n",
      "None\n",
      "-----------------------FOLD 2---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.1647\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.320567\n",
      "Default score: [-0.28004566]\n",
      "Accuracy is: 0.8933029192403311\n",
      "F1 is: 0.8973365998519525\n",
      "None\n",
      "-----------------------FOLD 3---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16417\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319121\n",
      "Default score: [-0.27862427]\n",
      "Accuracy is: 0.8943281133864726\n",
      "F1 is: 0.8983359964616141\n",
      "None\n",
      "-----------------------FOLD 4---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16399\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319278\n",
      "Default score: [-0.27897348]\n",
      "Accuracy is: 0.8936873670451342\n",
      "F1 is: 0.8977774593526966\n",
      "None\n",
      "-----------------------FOLD 5---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16517\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.320247\n",
      "Default score: [-0.28104736]\n",
      "Accuracy is: 0.8933712655167406\n",
      "F1 is: 0.8974668137124565\n",
      "None\n",
      "-----------------------FOLD 6---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16433\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319125\n",
      "Default score: [-0.27708969]\n",
      "Accuracy is: 0.8942939402482678\n",
      "F1 is: 0.8982499505732425\n",
      "None\n",
      "-----------------------FOLD 7---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16482\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.32092\n",
      "Default score: [-0.28160268]\n",
      "Accuracy is: 0.8928244953054651\n",
      "F1 is: 0.8967128701550278\n",
      "None\n",
      "-----------------------FOLD 8---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16444\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "#using the make_submission_file method to make predicction and create a submission file\n",
    "\n",
    "func_.make_submission_file(filename='testing6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "SmxsGsBWmtAp",
    "outputId": "66e36d27-2645-4d7c-c81f-01116a353e31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000    97125\n",
       "30000    24010\n",
       "65030     9027\n",
       "70000     4754\n",
       "99000      988\n",
       "80000      456\n",
       "90000      401\n",
       "86000       25\n",
       "Name: # lithology, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/testing4.csv')\n",
    " \n",
    "a['# lithology'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "-4DZpnEReqb9",
    "outputId": "9740c84a-4d27-4a53-c731-8827f81425c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000    94447\n",
       "30000    24831\n",
       "65030    10489\n",
       "70000     5817\n",
       "80000      610\n",
       "99000      511\n",
       "90000       63\n",
       "86000       16\n",
       "88000        2\n",
       "Name: # lithology, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv('/content/drive/My Drive/FORCE-Lithology-Prediction/testing6.csv')\n",
    " \n",
    "a['# lithology'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mu6R4oHsewc8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FORCE_Submission_File.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
